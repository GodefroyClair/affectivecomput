---
title: "Mesures physiologiques de joueurs de jeu vidéo (2)"
subtitle: "Deuxième partie : analyse statistique et fouille des données"
author: "Godefroy Clair"
date: "Monday, July 13, 2015"
output:
  pdf_document:
    includes:
      in_header: C:/Users/Godefroy/Google Drive/ProjetPsychoGame/header.tex 
    fig_caption: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document:
    css: C:/Users/Godefroy/Google Drive/ProjetPsychoGame/style/gamer-expe.css
    number_sections: yes
    toc: yes
  word_document: default
---

```{r, echo=FALSE, cache= FALSE ,comment='', warning=F,message=F}
##Laisser le cache à False, sinon les bibliothèques ne sont pas chargées...
require(knitr)
require(plyr)
require(ggplot2)
require(reshape2)
#require(scatterplot3d)
require(grDevices)
require(kohonen)
require(ggdendro)
require(grid)
```

```{r, echo = F, cache=F}

mac.gd.path <- "/Users/godot/githubRepos/"
hp.gd.path <- "C:/Users/Godefroy/"


if(Sys.info()['nodename']=="THIM")gd.path <- hp.gd.path else
  gd.path <- mac.gd.path
data.path <- paste(gd.path,"affectiveComputing/data",sep = "/")
```

```{r, echo = F, cache=F}

rda.save <- paste(data.path ,"data-frame-all-expe.Rda",sep = "/")
load(rda.save)
```

\newpage

#Introduction

Comme nous avions conclu dans la partie précédente en soulevant quelques doutes concernant *certaines* mesures issues des expériences, nous allons procéder à une selection des expériences qui nous semblent en accord avec les plages de valeurs attendues. Nous procéderons ensuite à une fouille statistique sur cette selection.


#Selection des données

Nous avons à notre disposition 12 expériences nommées 'AB', 'CLP', 'CW', 'DA', 'FS1', 'HL', 'LM', 'PCo', 'PCo2', 'PCo3', 'DE' et 'ST'.
Nous allons mettre de côté celles dont les mesures nous paraissent trop incertaines pour pouvoir être utlisées. Dans un second temps, il sera possible d'envisager de faire une selection par variable : au lieu de supprimer une expérience complète, on ne supprime que les variables à écarter.

Nous proposons d'écarter les expériences suivantes :
*'FS1' à cause de la variable respiration
*'LM' à cause de la variable respiration
*'HL' à cause de la variable activité électrodermale (transpiration)
*'ST' à cause de la variable activité électrodermale (transpiration)
*'DE' car les données de cette expérience n'ont pas encore pu être vérifiée

Il nous reste donc les expériences  'AB', 'CLP', 'CW', 'DA', 'PCo', 'PCo2', 'PCo3' et 'DE', soit 8 expériences.

```{r}
df.selec <- df.all[!(df.all$nom.experience %in% c("FS1","LM","HL","ST")),]
list.expe.selec <- c("AB", "CLP", "CW", "DA", "PCo", "PCo2", "PCo3","DE")
```


#Fouille des données 

Nous allons commencer par fournir quelques statistiques et graphiques pour se donner une vue d'ensemble.

##Quelques statistiques descriptives


Quelques statistiques pour synthétiser l'ensemble données :

```{r,echo=FALSE,cache=F, fig.height=3, fig.width=3}
opts_chunk$set(results='asis')
#kable allows a petty print of a table
kable(summary(df.selec[,3:6]), digits=2)
opts_chunk$set(results='hold')
```

Nous ajoutons la déviation standard par variable:
```{r,echo=FALSE}
sapply(df.selec[,3:6],sd)
```

##Quelques représentations graphiques

```{r, echo=F, cache=F}
#Utilis par la suite...

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }
 
 if (numPlots==1) {
    print(plots[[1]])
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
       
    # Make each plot, in the correct location
    for (i in 1:(numPlots)) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
     
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
      
    }
  }
}

```

Nous allons proposer pour chaque variable trois histogrammes différents. Ils permettent de se donner une idée de la répartition globale des données :

-Le premier superpose à un histogramme classique, une courbe de densité. Il permet de "lisser" les variations et facilite le rapprochement avec une distribution de variable aléatoire. 

-Le second histogramme nous montre comment se décompose chacunr des barres de l'histogramme entre les 4 quart temps.

-Dans le troisième, les variations entre les barres sont gommées pour ne laisser voir que la répartition de des données entre les 4 goupes pour chaque intervale.

```{r,echo=FALSE, cache=F, warning=F}

make.histo = function(col,bin){
  #histo avec ggplot
  g <- ggplot(df.selec, aes_string(col, fill = "quart.temps"))
  
  #histogramme avec densité
  g1 <- g + geom_histogram(aes(y=..density..),binwidth = bin, colour="black", fill="white") + geom_density(alpha=.2, fill="#FF6666") + theme(axis.title.x=element_blank(),legend.text=element_text(size=25),legend.title=element_text(size=25),axis.title.y=element_text(size=25))
  
  
  #histo divisé par quart-temps
  g2 <- g +  geom_bar(binwidth = bin) + theme(axis.title.x=element_blank(),legend.text=element_text(size=25),legend.title=element_text(size=25),axis.title.y=element_text(size=25))

  
  #histo divisé par quart-temps avec une longueur constante pour chaque rectangle
  g3 <- g + geom_bar(position="fill",binwidth = bin) + theme(axis.title.x=element_blank(),legend.text=element_text(size=25),legend.title=element_text(size=25),axis.title.y=element_text(size=25))
  
  
  multiplot(g1,g2,g3,cols=1)
  
}

```

\newpage

###Pour la respiration :

```{r, echo=F, cache=F, warning=F, fig.height=30, fig.width=25, fig.cap="histogrammes respiration"}
make.histo("respiration",.75)

```
\newpage

###Pour l'activité electrodermale :

```{r, echo=F, cache=F, fig.height=30, fig.width=25, fig.cap="histogrammes transpiration"}

make.histo("activite.electrodermale",.25)

```
\newpage

###Pour la température :

```{r, echo=F, cache=F, fig.height=30, fig.width=25, fig.cap="histogramme température"}
make.histo("temperature",.25)

```
\newpage

###Pour la fréquence cardiaque :

Différents histogrammes pour la fréquence cardiaque :

```{r, echo=F, cache=F, fig.height=30, fig.width=25, fig.cap="histogrammes fréquence cardiaque"}
make.histo("frequence.cardiaque",1)
```
\newpage

##Première fouille des données


###Cartes auto-adaptatives (SOM)


Nous allons procéder à une simulation grâce à la bibliothèque "kohonen" implémentée sous 'R'. Après avoir centré et réduit l'ensemble des données, nous allons créer une carte de Kohonen de taille 40x40 (ie une grille de 40 neurones - ou référents - par 40) avec une topologie hexagonal.

La bibliothèque Kohonen sous R[^1] permet de créer une telle carte grâce à la fonction "som"[^2]. La distance utilisée est la distance euclidienne. Le nombre d'itération peut être fixé par le modélisateur. Par défaut, la "courbe d'apprentissage", indiquant la pondération donnée au poids que chaque nouvelle unité d'un neurone sur la "similarité" de ce neurone, diminue linéairement de 0.05 à 0.01 à chaque itération et le "radius" de voisinage : la taille initiale du voisinage de chaque neurone et la fonction caractérisant son évolution.

Comme nous voulons donner autant de poids à toutes les variable, nous centrons et réduisons les 4 variables.

```{r, echo=TRUE, cache=F}

#obtenir un dégradé de couleurs de bleu à rouge
coolBlueHotRed <- function(n, alpha = 1) {rainbow(n, end=4/6, alpha=alpha)[n:1]}

#les variables sont centrées et réduites
df.sc <- scale(df.selec[,2:5])

#verification en prenan lees 1eres lignes
kable(head(df.sc))

set.seed(77)
#calcul de l'a cart'algorithme d'attiribution des données aux neurones
#rlen permet de préciser le nombre d'itérations
som1 <- som(data = df.sc, grid = somgrid(30, 30, "hexagonal"), rlen=75)

#donne des informations sur la carte
print(som1)
```

####Vérification

Un certain nombre de graphiques sont disponibles pour aider à juger du bon déroulement de  l'algorithme et ensuite permettre l'interprétation et la visualisation des résultats. La plupart ont une base commune : chaque neurone est représenté sur la carte par un disque ayant certaines caractéristiques esthétiques (couleurs, transparences...) et géométriques (courbes, camemberts...) qui permettent de visualier certaines propriétés ou valeurs associées à ce neurone. A noter que, par convention, on consdière que la lecture se fait de gauche à droite et de bas en haut. Ainsi, le *premier* référent est celui en bas à gauche et le *dernier* se situe en haut à droite.

Le premier graphique est le "Training Progress". Il donne l'évolution de la distance moyenne des vecteurs de données au neurone auquel elles ont été attribuées. 

```{r, echo=TRUE, cache=F, ,fig.cap="carte du progrès d'apprentissage"}
plot(som1, type="changes", main="")
```

L'apprentissage semble s'être correctement déroulé : après avoir continuement diminuée, la distance moyenne des données aux neurone a atteint un plateau un peu avant la 40ème itération. Il ne semble donc pas nécessaire de faire plus d'itérations.

\newpage

Une autre carte utile pour voir comment s'est déroulé l'apprentissage est la carte "Node Counts" qui permet de visualiser comment se sont distribués les données entre les neurones : y a-t-il des neurones qui ont capté une grande partie des données ou la répartition est plus ou moins égalitaire ? y a-t-il des neurones qui ne captent pas de données ?

```{r, echo=TRUE, cache=F, fig.cap="carte de comptages des données captées", out.width=".9\\paperwidth"}
plot(som1, type="count", main= "")
```


Nous voyions (figure 4) que la plupart des neurones se sont vu attribuer un nombre de données compris entre 100 et 1000. Autre point intéressant, un certain nombre de neurones n'ont aucune donnée associée (en gris sur le graphique) et l'ensemble de ces neurones "vides" forment des frontières qui séparent les données en 5 groupes. 

\newpage

Une autre carte intéressante est la carte dite de "qualité" (figure 5) qui montre la distance de chaque neurone aux données qui lui ont été attribuées.

```{r, echo=TRUE, cache=F, fig.cap="carte de \"qualité\" : distance moyenne des données aux neurones  "}
plot(som1, type="quality", palette.name = coolBlueHotRed, main = "")

```

Nous voyions que la distance est *faible*, assez *uniforme* et montrant peu de cas "aberrants" ; ce qui laisse là aussi présager d'une bonne répartition des données. On remarque aussi que les neurones qui sur la carte précédente n'avaient pas de données associées n'ont pas de distance (couleur grise).

IL est intéressant de comparer aux premières cartes de Kohonen réalisées à partir d'expériences particulières[^3] avec des données non-traitées. On voit que, si dans le cas présent la distance est globalement bien inférieure (<0.01) et que seuls deux neurones voient cette distance moyenne être supérieure à 0.05 (pour les expériences AB et LM, nous trouvions 10% des neurones ayant une distance moyenne à ses données de plus de 0.5 (et presque 5% avec une distance supérieure à 1).

\newpage

####Interprétation

La carte suivante (figure 6) permet de passer à l'interprétation : cette carte dite des "codebook vectors" indique quelles sont les caractéristiques de la donnée moyenne associées à chaque neurone. On peut ainsi parler de la carte d'identité de chaque neurone. Pour représenter cela dans le graphe, tout neurone de la carte (représenté par le biais d'un disque) contient des "parts de tarte" plus ou moins larges qui se partage ainsi la surface de ces disques. Ils représentent la valeur moyenne des données captées par le neurone. 
Ainsi, si la surface de la "part de tartes" représentant la température est la plus grande (comme c'est le cas au nord-est de la carte), alors cela signifie que, aux instants capturées par ce neurone, la température cutanée était relativement élevée.

\newpage
\newgeometry{left=0cm, right=0cm,bottom=4cm}
\blandscape

```{r, echo=F, cache=F, fig.height=8.5, fig.width=8.5,fig.cap="carte de l'identité des neurones", out.width="1\\paperwidth"}
par(xpd=F)
par(mai=  c(0, 0, 0, 0))
plot(som1, main = "", type="codes",labels = NULL) 
#legend("bottomright", "")
#legend(2.8,0,c("temperature", "transpiration"), yjust = 0)
```
\elandscape
\restoregeometry

Du fait du nombre de neurones, la carte est difficilement lisible.On peut utliser certaines astuces pour remédier à ce problème.

D'abord, on peut travailler avec une carte plus petite. (voir figure 7)

```{r, echo=F, cache=F}

#obtenir un dégradé de couleurs de bleu à rouge
coolBlueHotRed <- function(n, alpha = 1) {rainbow(n, end=4/6, alpha=alpha)[n:1]}

#les variables sont centrées et réduites
df.sc <- scale(df.selec[,2:5])

#verification en prenan lees 1eres lignes
kable(head(df.sc))

set.seed(77)
#calcul de l'a cart'algorithme d'attiribution des données aux neurones
#rlen permet de préciser le nombre d'itérations
som2 <- som(data = df.sc, grid = somgrid(10, 10, "hexagonal"), rlen=75, keep.data = T)

```

La carte suivante (figure 7) a été créée avec une topologie de 100 (ie 10 * 10) neurones.
 
```{r, echo=TRUE, cache=F, fig.height=15, fig.width=15, out.width=".9\\paperwidth", fig.cap="carte des neurones (10x10)"}
plot(som2, main = "carte SOM") 
```

Vu le petit nombre de variables, nous pouvons aussi proposer des cartes "heatmaps" qui permettent de regarder la valeur moyenne associée à chaque neurone valeur par valeur :

Heatmap de la respiration : voir figure 8

```{r, echo=F, cache=F, fig.height=8, fig.width=8, fig.cap="carte heatmap de la respiration", warning=F}
plot(som1, type = "property", property = som1$codes[,1], main="", palette.name=coolBlueHotRed)
```

Heatmap de l'activité électrodermale : voir figure 9

```{r, echo=F, cache=F, fig.height=8, fig.width=8, fig.cap="carte heatmap de l'activité électrodermale", warning=F}
plot(som1, type = "property", property = som1$codes[,2], main="", palette.name=coolBlueHotRed)
```

Heatmap de la température : voir figure 10

```{r, echo=F, cache=F, fig.height=8, fig.width=8, fig.cap="carte heatmap de la température"}
plot(som1, type = "property", property = som1$codes[,3], main="", palette.name=coolBlueHotRed)
```

Heatmap de la fréquence cardiaque : voir figure 11

```{r, echo=F, cache=F, fig.height=8, fig.width=8, fig.cap="carte heatmap de la fréquence cardiaque", warning=F}
plot(som1, type = "property", property = som1$codes[,4], main="", palette.name=coolBlueHotRed)
```

Synthèse des heatmaps :

La température est un des facteurs les plus clivants, la respiration varie aussi beacoup entre un gros quart "sud-est" et le reste de la carte.
Il est intéressant de superposer température et respiration (voir figure 11) : les deux cartes semblent varier à l'opposé l'une de l'autre.

Heatmap de la respiration :

```{r, echo=F, cache=F, fig.height=4, fig.width=8, fig.cap="comparaison des \"heatmaps\" de transpiration et respiration"}
par(mfrow=c(1,2))
plot(som1, type = "property", property = som1$codes[,1], main="respiration", palette.name=coolBlueHotRed)
plot(som1, type = "property", property = som1$codes[,3], main="transpiration", palette.name=coolBlueHotRed)
```

visualisation des véritables valeurs associées aux neurones

Cela consiste à présenter la variable avant la normalisation. Cela demande sous R un travail préalable

```{r, echo=F, cache=F, fig.height=15, fig.width=15, fig.cap="heatmaps des variables non-normalisées", warning=F}
par(mfrow=c(2,2))
#respi
resp.unscaled <- aggregate(df.selec$respiration,by=list(som1$unit.classif), FUN=mean, simplify=TRUE)[,2]
plot(som1, type = "property", property = resp.unscaled, main="variable respiration non normalisée", palette.name=coolBlueHotRed)

#activite.electrodermale
transpi.unscaled <- aggregate(df.selec$activite.electrodermale,by=list(som1$unit.classif), FUN=mean, simplify=TRUE)[,2]
plot(som1, type = "property", property = transpi.unscaled, main="variable transpiration non normalisée", palette.name=coolBlueHotRed)

#température
temp.unscaled <- aggregate(df.selec$temperature,by=list(som1$unit.classif), FUN=mean, simplify=TRUE)[,2]
plot(som1, type = "property", property = temp.unscaled, main="variable température non normalisée", palette.name=coolBlueHotRed)

#frequence.cardiaque
freq.unscaled <- aggregate(df.selec$frequence.cardiaque,by=list(som1$unit.classif), FUN=mean, simplify=TRUE)[,2]
plot(som1, type = "property", property = freq.unscaled, main="variable freq. card. non normalisée", palette.name=coolBlueHotRed)

freq.unscaled <- aggregate(df.selec$frequence.cardiaque,by=list(som1$unit.classif), FUN=mean, simplify=TRUE)[,2]

par(mfrow=c(1,1))
```

Sur la figure 12, on peut voir que les 4 cartes se superposent et que l'on peut définir un certain nombre de zone où les variables varient peu.
\newpage

\newpage

Voyons avec la figure 14, comment les différentes expériences se partagent la carte en observant quelle est l'expérience majoritaire pour chaque neurone, celle dont les données sont les plus nombreux à lui avoir été associées.

\newgeometry{left=0.8cm, right=0.5cm,bottom=3cm,top=0cm}

```{r, echo=F, cache=F, fig.height=18, fig.width=18, fig.cap="experience majoritaire par neurone",out.width="0.95\\paperwidth"}

par(mfrow=c(1,1))

#créons une nvelle variable donnant le neurone associé à chaque données
#attention : certaines neurones n'ont pas de données associées dc ils n'apparaissent pas
df.selec$nom.neurone <- som1$unit.classif

#data frame du nombre d'individus par expe pour chaque neurone **non vide**
expe.par.neurone <- as.data.frame(with(df.selec, tapply(row.names(df.selec),list(neurone=nom.neurone,experience=nom.experience), length)))


#ajoutons les neurones manquants
#simplifier ??
list.neuro.vid <- which(! seq(1:900) %in% as.numeric(rownames(expe.par.neurone)))
nb.neuro.vid <- length(list.neuro.vid)
#df des neurones manquants
df.neuro.vid <- as.data.frame(matrix(nrow=nb.neuro.vid,ncol = ncol(expe.par.neurone)))
#prépa du rbind avec autre fdf
rownames(df.neuro.vid) <- list.neuro.vid
colnames(df.neuro.vid) <- colnames(expe.par.neurone)
#rbind & mis en ordre
expe.par.neurone <- rbind(expe.par.neurone,df.neuro.vid)
expe.par.neurone <- expe.par.neurone[order(as.numeric(row.names(expe.par.neurone))),]

                    
#expe.par.neurone

#supprimons les facteurs inutiles
expe.par.neurone <- expe.par.neurone[,list.expe.selec]
expe.par.neurone[is.na(expe.par.neurone)]<-0

#nom de l'experience la plus représentée par neurones
expe.major.par.neurone <- as.factor(list.expe.selec[max.col(expe.par.neurone)])
#put NA in line of neurone empty
expe.major.par.neurone[rowSums(expe.par.neurone)==0]<-NA
#test <- as.factor(rep(c("AB","CW","DA"),300))
#plot(som1, type = "property", property = as.numeric(test), main="expérience majoritaire par neurone")


#représentons cela graphiquement :
plot(som1, type = "property", property = as.numeric(expe.major.par.neurone), main="",  heatkey =F,palette.name=coolBlueHotRed)
text(x=som1$grid$pts[,1],y=som1$grid$pts[,2],labels=expe.major.par.neurone,cex = 1,font=2)


#RESTE
#exp.unscaled.mode <- aggregate(as.numeric(df.selec$nom.experience),by=list(som1$unit.classif), FUN=summary, simplify=TRUE)[,2]
#colnames(mat.freq) <- c("AB", "CLP", "CW", "DA", "PCo", "PCo2", "PCo3", "DE")

```


\restoregeometry

On remarque que chaque expérience se confine dans une partie de la carte, ce qui laisserait entendre que l'apprentissage a pris en compte les particularités de chaque expérience dans son mapping des données aux neurones.

Nous pouvons confirmer cette impression grâce au tableau suivant qui, pour un échantillon de neurones tirés uniformémement parmi les 900, donne la répartition des expériences dont sont issues les données que chacun a capté.Nous en tirons 45, soit 5%. La première colonne nous donne le numéro du neurone tiré au hasard parmi les 900, les suivantes, le nombre de données issus de chaque expérience.

```{r, echo=F, cache=F, fig.height=8, fig.width=8, fig.cap="Répartition des expériences pour un échantillon de neurones", warning=F}
kable(expe.par.neurone[sample(x=1:length(expe.major.par.neurone),size = 45),])
```
 
Nous observions bien qu'une grande partie des neurones ont capté des données liées à une seule expérience. En fait, par un calcul simple, on peut trouver que `r sum.exp.neuro <- with(expe.par.neurone, rowSums(expe.par.neurone)); multi.expe <-length(expe.major.par.neurone)-length(which(apply(expe.par.neurone[,1:length(list.expe.selec)]==sum.exp.neuro,1, function(x) (sum(x)==0))));multi.expe`, soit `r  round(multi.expe/length(expe.major.par.neurone)*100)`% des neurones n'ont capté que les données d'une expérience.

Par ailleurs, concernant le graphique montrant l'expérience majoritaire par neurone (figure 14), il est intéressant de noter que les zones des expériences PCo, PCo3 et Pco3 (qui ont été réaliées sur une même personne) sont assez fortement entremêlés ou en tout cas pas aussi démarqués qu'avec d'autres individues.

Dans ce cadre, il est aussi intéressant de reprendre le tableau précédent mais cette fois en regardant les neurones associés à des données non-issues d'une unique expérience.

```{r, echo=F, cache=F, fig.height=8, fig.width=8, fig.cap="Répartition des expériences pour les neurones en ayant captés plusieurs", warning=F}
kable(expe.par.neurone[(which(apply(expe.par.neurone[,1:length(list.expe.selec)]==sum.exp.neuro,1, function(x) (sum(x)==0)))),])
```

Nous voyions là-aussi que les neurones ayant captées des données issues de différentes expériences sont pour leur grande majorité des données des expériences de l'individu "PCo". Ainsi, les neurones mettent ensemble les données d' expériences issues d'un même individu  et seulement celle-ci. AIns, il semble que les cartes de Kohonen soit capable avec ces données de distinguer des individus.

L'intéret à terme de ce fait peut être grand (sur la question de la prédiction des particularités émotionnelles *indivuelles* notamment) mais puisque nous cherchons à associer des neurones non par une personne particulière mais plus à un type de mesure (qui serait lié à une émotion), il nous faudra aborder le problème d'une manière différente.

Comme nous avons un certain nombre d'expériences non encore exploitées, nous pensons qu'en les intégrant à ces cartes, nous espérons que les différentes zones ne correspondent pas uniquement à un individu mais à un type d'individu. Le fait que les expériences issues d'un même individu soit "reconnues" pas la carte comme étant des données similaires nous confortent un peu dans cette possibilité.



####Projection par quart-temps

Si nous arrivons à découper la carte entre zones représentant des individus types, il faut ensuite caractériser pour chaque zone à quels périodes correspond chaque neurone : y a-t-il des périodes prolongées qui sont captées par certains neurones ?

Dans un premier temps, pour le savoir,nous pouvons faire une projection par quart-temps en ajoutant sur la carte le quart-temps majoritaire dont sont issues les expériences.

\newgeometry{left=0.8cm, right=0.5cm,bottom=3cm,top=0cm}

```{r, echo=F, cache=F, fig.height=18, fig.width=18, fig.cap="quart temps majoritaire par neurone",out.width="0.95\\paperwidth"}

par(mfrow=c(1,1))

#créons une nvelle variable donnant le neurone associé à chaque données
#attention : certaines neurones n'ont pas de données associées dc ils n'apparaissent pas
df.selec$nom.neurone <- som1$unit.classif

#data frame du nombre d'individus par expe pour chaque neurone **non vide**
expe.par.qr.tps <- as.data.frame(with(df.selec, tapply(row.names(df.selec),list(neurone=nom.neurone,experience=quart.temps), length)))


#ajoutons les neurones non associés à un quart-temps (car "vides")
list.neuro.vid <- which(! seq(1:900) %in% as.numeric(rownames(expe.par.qr.tps)))
nb.neuro.vid <- length(list.neuro.vid)
#df des neurones manquants
df.neuro.vid <- as.data.frame(matrix(nrow=nb.neuro.vid,ncol = ncol(expe.par.qr.tps)))
#prépa du rbind avec autre fdf
rownames(df.neuro.vid) <- list.neuro.vid
colnames(df.neuro.vid) <- colnames(expe.par.qr.tps)
#rbind & mis en ordre
expe.par.qr.tps <- rbind(expe.par.qr.tps,df.neuro.vid)
expe.par.qr.tps <- expe.par.qr.tps[order(as.numeric(row.names(expe.par.qr.tps))),]

                    
#expe.par.neurone

qr.tps <- c("1er", "2eme", "3eme", "4eme")
expe.par.qr.tps <- expe.par.qr.tps[,qr.tps]
expe.par.qr.tps[is.na(expe.par.qr.tps)]<-0

#nom de l'experience la plus représentée par qr.tps
expe.major.par.qr.tps <- as.factor(qr.tps[max.col(expe.par.qr.tps)])
#put NA in line of quart tps empty
expe.major.par.qr.tps[rowSums(expe.par.qr.tps)==0]<-NA
#test <- as.factor(rep(c("AB","CW","DA"),300))
#plot(som1, type = "property", property = as.numeric(test), main="expérience majoritaire par qr.tps")


#représentons cela graphiquement :
plot(som1, type = "property", property = as.numeric(expe.major.par.qr.tps), main="",  heatkey =F,palette.name=coolBlueHotRed)
text(x=som1$grid$pts[,1],y=som1$grid$pts[,2],labels=expe.major.par.qr.tps,cex = 1,font=2)


#RESTE
#exp.unscaled.mode <- aggregate(as.numeric(df.selec$nom.experience),by=list(som1$unit.classif), FUN=summary, simplify=TRUE)[,2]

```

\restoregeometry

Nous pouvons recouper expériences et quart temps :

\newgeometry{left=0.8cm, right=0.5cm,bottom=3cm,top=0cm}

```{r, echo=F, cache=F, fig.height=18, fig.width=18, fig.cap="expérience et quart temps (couleur) majoritaire par neurone",out.width="0.95\\paperwidth"}
plot(som1, type = "property", property = as.numeric(expe.major.par.qr.tps) ,  main="",  heatkey =F,palette.name=coolBlueHotRed)
text(x=som1$grid$pts[,1],y=som1$grid$pts[,2],labels=expe.major.par.neurone,cex = 1,font=2)

```
\restoregeometry


Parmi les `r nrow(expe.par.qr.tps)` neurones, combien de neurones captent des données issues de plusieurs quart temps ?

```{r, echo=F, cache=F, fig.height=8, fig.width=8, fig.cap="Répartition des expériences pour les neurones en ayant captés plusieurs", warning=F}
sum.exp.qr.tps<- with(expe.par.qr.tps, rowSums(expe.par.qr.tps));
#quelles neurones ont une de 4 colonnes égales au totales (ce qui signifie que les autres sont à zéro)
table.neur.multi.qr.tps <- expe.par.qr.tps[(which(apply(expe.par.qr.tps==sum.exp.qr.tps,1, function(x) (sum(x)==0)))),]
```
Il y `r nrow(table.neur.multi.qr.tps)` neurones qui captent des données issues d'au moins deux quart-temps. Ce qui est donc une large majorité...


####Projection du temps sur les référents


```{r, echo=F, cache=F, fig.height=18, fig.width=18, fig.cap="quart temps majoritaire par neurone",out.width="0.95\\paperwidth"}

#construction d'une variable numérique représentant le temps écoulé
df.selec$tps.ecoule <- as.numeric(df.selec$date)


#transltation des valeurs pour chaque expérience de manière à ce que le 1er instant soit 0.
trash <-sapply(list.expe.selec,function(nom.exp){ df.selec[df.selec$nom.experience==nom.exp,]$tps.ecoule <<- df.selec[df.selec$nom.experience==nom.exp,]$tps.ecoule -  min(df.selec[df.selec$nom.experience== nom.exp,]$tps.ecoule) })

rm(trash)

df.selec$tps.ecoule.norm <- df.selec$tps.ecoule

#divsion des valeurs pour chaque expérience de manière à ce qu'elle soit compris entre 0 et 1.
trash <-sapply(list.expe.selec,function(nom.exp){ df.selec[df.selec$nom.experience==nom.exp,]$tps.ecoule.norm <<- df.selec[df.selec$nom.experience==nom.exp,]$tps.ecoule.norm /  max(df.selec[df.selec$nom.experience== nom.exp,]$tps.ecoule.norm) })

rm(trash)

#time.com <- colorRamp(c('blue', 'red'))(df.selec$tps.ecoule.norm)


#représentons cela graphiquement :
#A REVOIR !!
#plot(som1, type = "mapping", property = "df.selec$tps.ecoule.norm", main="",  heatkey =F,palette.name=coolBlueHotRed)
#text(x=som1$grid$pts[,1],y=som1$grid$pts[,2],labels=expe.major.par.qr.tps,cex = 1,font=2)
#colorRamp(c('blue', 'red'))(as.numeric(df.selec$tps.ecoule.norm))
#plot(som1, type = "property", property =,  main=df.selec$tps.ecoule.norm,  heatkey =F,palette.name=coolBlueHotRed)

```


Sur chacun des graphiques de la figure 17,  nous pouvons voir comment évolue les données d'une expérience entre les différentes neurones.

\newgeometry{left=0.5cm, right=0.5cm,bottom=3cm,top=0cm}

```{r, echo=F, cache=F, fig.height=18, fig.width=18, fig.cap="déplacement des données entre les neurones par expérience",out.width="0.95\\paperwidth"}

list.gg <- lapply(list.expe.selec, function(nom.expe) { 
  g<- ggplot(data = df.selec[df.selec$nom.experience==nom.expe,], aes (tps.ecoule.norm,nom.neurone)) + 
    geom_line(aes(colour = tps.ecoule.norm)) + 
    scale_color_continuous(name="temps écoulé)", low = "yellow",high = "red") + 
    xlab("proportion du temps écoulé") + 
    ylab("numéro du neurone") + ggtitle(paste("déplacement des données de",nom.expe,"entre les neurones",sep = " "))+ 
    scale_y_continuous(breaks=seq(0,900,by = 10))
  })

multiplot(list.gg[[1]],list.gg[[2]],list.gg[[3]],list.gg[[4]],list.gg[[5]],list.gg[[6]],list.gg[[7]],cols = 2)

#do.call("multiplot", as.list(list.gg,cols=2)) 

```
\restoregeometry

\newpage


Conclusion :
Nous avons montré que les cartes de Kohonen permettent de distinguer les différentes expériences. 




[^1]:voir https://cran.r-project.org/web/packages/kohonen/kohonen.pdf
[^2]:voir http://www.jstatsoft.org/v21/i05/paper
[^3]:voir les fichiers *experience_AB.html* et *experience_LM.html* dans le dossier joint en annexe.