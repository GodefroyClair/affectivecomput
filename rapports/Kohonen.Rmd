---
title: "Presentation graphiques pour l’analyse de données"
subtitle: "comment améliorer l'étiquetage des cartes de Kohonen"
author: "Godefroy Clair"
date: "28 août 2016"
output:
  pdf_document:
    includes:
      in_header: ../style/header.tex 
    fig_caption: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document:
    css: ../style/simple-report.style.css
    number_sections: yes
    toc: yes
  word_document: default
---

```{r, include=FALSE}
df_type <- "not_normed" #"indiv" or "not_normed"
```
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(plyr)
library(ggplot2)
library(gtable)
library(kohonen)
library(tidyr)
library(signal)
library(dplyr)
library(purrr)
library(testthat)
library(stringr)
library(ggradar)
library(purrr)
library(reshape2)
library(lubridate)
library(data.table)
#instal ggradar : devtools::install_github("ricardo-bion/ggradar", dependencies=TRUE)
# configured to work on a Mac, change directory to Unix or Windows
#download.file("https://dl.dropboxusercontent.com/u/2364714/airbnb_ttf_fonts/Circular Air-Light 3.46.45 PM.ttf", "/Library/Fonts/Circular Air-Light 3.46.45 PM.ttf", method="curl")
#extrafont::font_import(pattern = 'Circular', prompt=FALSE)
suppressPackageStartupMessages(library(dplyr))
library(scales)
theme_set(theme_bw(24))
```

#Episode précédent : le nettoyage des données 

Pour arriver à l'étape de l'étiquetage des données, un long travail préalable a été nécessaire...
Ce travail est constitué de trois phases :

- l'importation de données "valides".
- le nettoyage des données.
- la décomposition des données en information.
- l'apprentissage statistique grâce aux cartes de Kohonen dites "auto-organisatrices".

## Importation

Il a fallu choisir les expériences dont les données étaient utilisables. Pour cela, un travail d'observation et de questionnement a été fait (voir rapport "rcp209_2").

## Le nettoyage

Le nettoyage a surtout porté sur une variable : la respiration. Ce choix est en partie du à un manque de temps mais aussi du fait de la qualité de cette mesure qui rendait peu exploitable les données brutes de respiration.
On peut dire que le travail fait sur la respiration était plus urgent que le nettoyage des autres données et en même temps un exemple du travail à réaliser sur les autres variables...
On peut apparenter le travail réalisé ici à celui d'un detective : sans connaissance prélable, armé uniquement de données et d'outils de visualisation, chercher à données sens aux différentes régularités et irréguralités que l'on observe dans les données.

## La décomposition

Ce travail est indissociable du travail de nettoyage : le nettoyage nous a amené à trouver dans les données différents phénomènes simples qu'il a fallu extraire des données brutes.
Par exemple, dans un certain nombre d'expériences, il y a un temps pendant laquelle la ceinture mesurant la respiration semble se déplacer vers la taille est la plus fine. 
Nous avons donc du "extraire" cet effet. Plusiseurs options ont été envisagées (regression exponentiel, polynomiale, par fenêtre...) mais finalement appliquer un filtre passe-bas a donné les meilleurs résultats.
GRAPHIQUE RESULTAT
Un autre effet à prendre en compte, le caractère cyclique des données qui recèle de nombreuses informations : la fréquence de la respiration et surtout son évolution (le changement de durée d'un cycle de respiration), l'amplitude, ...
GRAPHS
Il y a aussi l'existence d'un bruit dans la mesure qu'il faut nettoyer. Là encore, les filtres de Fourrier (passe-bas cette fois) permettent de supprimer ces phénomènes parasitaires dans la recherche d'états émotionnels
AJOUTER DES GRAPHS

GRAPHIQUE DES CAS D'APPRENTISSAGE DS LA MESURE DE RESPIRATION




###Réflexion sur cette décomposition

La question de savoir s'il faut décomposer une variable en amont du travail d'apprentissage est une question complexe dans le cadre des apprentissages non-supervisées. Elle peut être considérée comme une forme d'assistance au travail des cartes de Kohonen alors que, dans l'idéale, l'algorithme devrait justement permettre de distinguer les différents phénomènes. En un sens, on s'éloigne des canons de l'apprentissage non-supervisée en introduisant plusieurs hypothèses apriori alors que l'idée de ces algorithmes est de les minimiser au maximum.
Tout ceci est vrai mais on peut avancer deux arguments : 

- D'abord il est inutile de demander à l'aglorithme des phénomènes que l'on peut observer "à l'oeil nu" : on perdrait de précieuses ressources de calcul dans ce travail. 
- Mais surtout, c'est l'essence même du travail de nettoyage  au sens large (dont fait partie la décomposition des phénomènes cachés dans les données) que de faire des hypothèses basées sur visualisation et de modifier les données en conséquence. Aucun travail d'apprentissage statistique ne peut se passer de ce travail déductif préalable sur les données qui doit aboutir à leur modification (s'appuyant sur des inférences).

Par contre, là où ce travail doit être amélioré, c'est sur la question de comment prendre en compte les conséquences de ce nettoyage. En effet, on introduit un biais potentiel puisque les variables issues de la décomposition vont être prises en compte dans les cartes de Kohonen selon une pondération qui est liées à la métrique choisie. Or, cette métrique reflète souvent plus l'hétérogénéité des données suivant chaque variable que leur valeur intrinsèque. Par exemple, avec la métrique basée sur les normes euclidiennes, les variables montrant A FINIR (CITER travaux Anastase)

En un sens dans l'état actuel, nous restons au milieu du gué : nous introduisons des hypothèses apriori qui ont des conséquences sur le résultat mais nous laissons au hasard le choix de comment ces hypothèses influencent le calcul.
Des travaux récents [CHRARANTONIS??] ont cherché à répondre à cette question en introduisant de nouvelles métriques....

## L'apprentissage statistique : les cartes de Kohonen

Une fois, les données décomposées en variables réputées, on peut lancer l'algorithme des cartes de Kohonen.

### Les résultats de l'apprentissage

Une difficulté est survenu du fait que l'apprentissage par défaut amené à un courbe d'apprentissage "coudée". Ce coude signifie généralement que l'apprentissage s'arrête à un minimum local... 

Finalement, on voit que l'on passe le coude, même s'il en reste un petit...

```{r, echo = F, message = F, cache = F}
load(verbose = T, file = "../data/som1.RDa")
load(verbose = T, file = "../data/som2.RDa")
load(verbose = T, file = "../data/df_stat_scale_scale.RDa")
load(verbose = T, file = "../data/df_stat_scaled.RDa")
load(verbose = T, file = "../data/df_selec.RDa")
#plot of the learning curve of kohonen maps (distance )
#a changer
#df_stat <- df_stat_scaled #df_stat_scaled ou df_stat_scale_scale
if(df_type == "indiv") {
  df_stat <- df_stat_scaled
  som <- som1 #som1 ou som2
} else if (df_type == "not_normed") {
  df_stat <- df_stat_scale_scale
  som <- som2 #som1 ou som2
}
```

```{r, echo = F, message = F, cache = F}
plot(som, type="changes", main="carte du progrès d'apprentissage")
```

 

```{r, echo = F, cache = F}
plot(som, type="count", main= "carte de comptages des données captées")

```


```{r, echo = F, cache = F}
#obtenir un dégradé de couleurs de bleu à rouge
coolBlueHotRed <- function(n, alpha = 1) {rainbow(n, end=4/6, alpha=alpha)[n:1]}
plot(som, type="quality", palette.name = coolBlueHotRed, main = "carte de \"qualité\" : distance moyenne des données aux neurones")
```

### Quelques autres graphiques standards

A commenter...
```{r, echo = F, cache = F}
#TODO : heatmap par variable
plot(som, type = "property", property = som$codes[,1], main="carte heatmap de l'activité électrodermale", palette.name=coolBlueHotRed)
```


```{r, echo = F, cache = F}
plot(som, type = "property", property = som$codes[,2], main="carte heatmap de temperature", palette.name=coolBlueHotRed)
```

```{r, echo = F, cache = F}
plot(som, type = "property", property = som$codes[,3], main="carte heatmap de freq card", palette.name=coolBlueHotRed)
```

```{r, echo = F, cache = F}
plot(som, type = "property", property = som$codes[,4], main="carte heatmap de respi nettoyée", palette.name=coolBlueHotRed)
```


```{r, echo = F, cache = F}
plot(som, type = "property", property = som$codes[,5], main="carte heatmap de min max", palette.name=coolBlueHotRed)
```


```{r, echo = F, cache = F}
plot(som, type = "property", property = som$codes[,6], main="carte heatmap de frequence", palette.name=coolBlueHotRed)
```

# Les nouveaux graphiques

## Les graphiques de type "multi-radars" et "multi-bar"

L'idée est de reprendre la carte des "codes" disponibles avec la bibliothèque standard de Kohonen mais en utilisant la grammaire des graphiques et en offrant ainsi plus de flexibilité.
Quelques résultats :

\newgeometry{left = 0cm, right = 0cm, top = 1cm, bottom = 3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte de l'identité des neurones", out.width="1\\paperwidth"}
#limit the nb of neurones we are working on to fasten the graph making process
#nb_group has to be 300 in the end
nb_group <- 300
nb_var <- 8
#make a df with the value of the variables associated with each neurone and the id of the neurone (group)
#it is easier to rescale everythng to ratio ([0-1]) with rescale
df_codes <- data.frame(som$codes) %>% add_rownames(var = "group")  %>% 
  mutate_each(funs(rescale), -group)  %>% head(nb_group)
#group needs to be numeric
df_codes$group <- as.numeric(df_codes$group)
# to be able to use geom_bar we need to untidy the data
melted_radar <- melt(df_codes, id = "group")

#we build the label such that each facetted graph has a number associated to it
##!! this is a patch, there must be a way to do this BETTER !!
base_rep <- 1:nb_group
rep <- rep("", nb_var - 1)
lab_rep <- purrr::map(base_rep, function(ele) { c(ele, rep)}) %>% unlist()
#ann_text <- data.frame(x=1.25, y=5, lab = as.character(lab_rep))

#titre des graphiques
base_titre <- "Carte d'identité des référents"

#theme that are used for the multiradar plot
radar_theme <-  theme(panel.margin.x = unit(0, "lines"), panel.margin.y = unit(0, "lines"), panel.border = element_rect(colour = rgb(1.0, 0, 0, 0.5), fill=NA, size=1),
        axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(),
        axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(),
        strip.background = element_blank(), strip.text.x = element_blank(),
        legend.title = element_text(colour="black", size=5,  face="bold"),
        legend.text = element_text(colour="black", size=5),
        legend.key = element_rect(size = 1),
        title = element_text(color = "blue", size =6, face = "bold"))

```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm, bottom=3cm}
\newpage
\blandscape

```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte de l'identité des neurones", out.width="1\\paperwidth"}
#length
ggplot(melted_radar, aes(x = variable, y = value, fill = variable)) %+% 
  facet_wrap( ~ group, ncol = 30)  %+% 
  geom_bar(stat="identity",position="dodge") %+%
  ylim(0,1) %+%
  geom_text(aes(x = 1.5, y = 1, label = lab_rep), colour="black", size = 1, inherit.aes=F) %+%
  coord_polar(start = 0)  %+%
  radar_theme %+%
  ggtitle(paste(base_titre, "(longueur)"))

```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm, bottom=3cm}
\newpage
\blandscape

```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte de l'identité des neurones", out.width="1\\paperwidth"}
#area
ggplot(melted_radar, aes(x = variable, y = value, fill = variable)) %+% 
  facet_wrap( ~ group, ncol = 30)  %+% 
  ylim(0,1) %+%
  geom_bar(width = 1 ,stat="identity",position="dodge") %+%
  geom_text(aes(x = 1.5, y = 1, label = lab_rep), colour="black",  size = 1, inherit.aes=F) %+%
  coord_polar()  %+% 
  radar_theme %+%
  ggtitle(paste(base_titre, "(aire)"))

```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm, bottom=3cm}
\newpage
\blandscape

```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte de l'identité des neurones", out.width="1\\paperwidth"}
#radar
ggplot(melted_radar, aes(x = variable, y = value, fill = variable)) %+% 
  facet_wrap( ~ group, ncol = 30)  %+% 
  geom_bar(stat="identity",position="dodge",width=.2) %+%
  ylim(0,1) %+%
  coord_polar(start = 0)  %+%
  geom_text(aes(x = 1.5, y = 1, label = lab_rep), colour="black", size = 1, inherit.aes=F) %+%
  radar_theme %+%
  ggtitle(paste(base_titre, "(radar)"))

```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm, bottom=3cm}
\newpage
\blandscape

```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte de l'identité des neurones", out.width="1\\paperwidth"}
#bar
ggplot(melted_radar, aes(x = variable, y = value, fill = variable)) %+% 
  facet_wrap( ~ group, ncol = 30)  %+% 
  geom_bar(stat="identity",position="dodge",width=.5) %+%
  ylim(0,1) %+%
  geom_text(aes(x = 1.5, y = 1, label = lab_rep), colour="black", size = 2, inherit.aes=F) %+%
  radar_theme %+%
  ggtitle(paste(base_titre, "(ligne)"))

```
\elandscape
\restoregeometry

\newgeometry{left=5cm, right=0cm,bottom=3cm}
\newpage

## Carte des données captées par neurones

```{r, echo = F, cache = F}
#==========================================#
## Carte des données captées par neurones ##
#==========================================#

#création d'un nouveau df montrant le nombre de données associées à chaque référent
#attention : certaines neurones peuvent ne pas avoir de données associées...

#on prend le df qui a servi au som
df_ref <- df_stat #inutile ??
df_ref <- df_stat #inutile ??
#on ajoute experience
df_ref$nom_experience <- df_selec$nom.experience
#et id_neurone
df_ref$id_neurone <- som$unit.classif
#on retire le superflu, on regroupe par pair id_neurone associé - nom_experience...
#...et on compte le nombre de données associé à chaque pair
df_count_neuro_expe <- df_ref %>% select(id_neurone, nom_experience) %>% 
  group_by(id_neurone, nom_experience) %>% summarise(count = n())

#on utilise tidyr::spread pour que chaque expe est sa colonne
expe_par_neurone <- df_count_neuro_expe %>% spread(nom_experience, count, fill = 0)

#reste algo précédent
#expe_par_neurone <- as.data.frame(with(df_ref, 
#                                       tapply(row.names(df_selec),
#                                              list(neurone=id_neurone,experience=nom.experience), 
#                                              length)))


#ajoutons les neurones manquants

#liste des neurones non présents
list_neuro_vid <- which(!seq(1:900) %in% as.numeric(expe_par_neurone$id_neurone))
nb_neuro_vid <- length(list_neuro_vid)
#df des neurones manquants
df_neuro_vid <- as.data.frame(matrix(nrow=nb_neuro_vid,ncol = ncol(expe_par_neurone)))
#prépa du rbind avec autre fdf
rownames(df_neuro_vid) <- list_neuro_vid
colnames(df_neuro_vid) <- colnames(expe_par_neurone)
df_neuro_vid$id_neurone <- list_neuro_vid
df_neuro_vid[is.na(df_neuro_vid)] <- 0
#bind_rows (not rbind !) & mis en ordre
expe_par_neurone <- bind_rows(expe_par_neurone,df_neuro_vid)
expe_par_neurone <- expe_par_neurone %>% arrange(id_neurone)

```


```{r, echo = F, cache = F}
##nom de l'experience la plus représentée par neurones
expe_major_par_neurone <- as.factor(colnames(expe_par_neurone[-1])[max.col(expe_par_neurone[-1])])
#put NA in line of neurone empty
expe_major_par_neurone[rowSums(expe_par_neurone)==0]<-NA
#test <- as.factor(rep(c("AB","CW","DA"),300))
#plot(som, type = "property", property = as.numeric(test), main="expérience majoritaire par neurone")

#representation graphique de l'expérience majoritaire par neurones :
plot(som, type = "property", property = as.numeric(expe_major_par_neurone), main="",  heatkey =F,palette.name=coolBlueHotRed)
text(x=som$grid$pts[,1],y=som$grid$pts[,2],labels=expe_major_par_neurone,cex = .5,font=2)
```

Graphique à réaliser avec ggplot...

\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte des expériences par neurone ", out.width="1\\paperwidth"}
## Graph des répartitions entre expériences par neurones
nb_group <- 100
nb_expe <- 8
base_rep <- 1:nb_group
base_rep <- nb_group*3:nb_group*4
rep <- rep("", nb_expe - 1)
lab_rep <- purrr::map(base_rep, function(ele) { c(ele, rep)}) %>% unlist()

# reprendre code pour recup expe_par_neurone
expe_par_neurone <- expe_par_neurone %>% head(nb_group) 
#expe_par_neurone$groups <- as.numeric(expe_par_neurone$groups)
#expe_par_neurone %>% filter(groups == 15)

#melted_expe_par_n <- melt(expe_par_neurone, id = "id_neurone") %>% 
#  group_by(id_neurone) %>% mutate(cumsum = cumsum(value)) %>% filter(variable != "id_neurone")

gathered_expe_par_n <- expe_par_neurone %>% gather(variable, value, -id_neurone) %>% 
  group_by(id_neurone) %>% mutate(cumsum = cumsum(value))

#gathered_expe_par_n %>% filter(id_neurone == 15)
  
expe_neuro_theme <-  theme(panel.margin.x = unit(0, "lines"), panel.margin.y = unit(0, "lines"), panel.border = element_rect(colour = rgb(1.0, 0, 0, 0.5), fill=NA, size=1),
        axis.text.x=element_text(size = 6), 
        axis.title.y=element_blank(), axis.text.y=element_text(size = 8), axis.ticks.y=element_blank(),
        strip.background = element_blank(), strip.text.x = element_blank(),
        legend.title = element_text(colour="black", size = 5,  face="bold"),
        legend.text = element_text(colour="black", size = 5),
        legend.key = element_rect(size = 1),
        title = element_text(color = "blue", size = 6, face = "bold"))

expe_neuro_theme2 <- theme(axis.text.y = element_text(size = 3, margin = margin(0, 0, 0, 0)))

ggplot(gathered_expe_par_n, aes(x = as.factor(id_neurone), y = value, fill = factor(variable))) %+% 
  geom_bar(stat = "identity") %+%
  #geom_text(aes(x = group + .2, y = cumsum - 15, label = value), size = 3, colour = "black", check_overlap = TRUE) %+%
  geom_text(aes(x = id_neurone - 0, y = (cumsum + cumsum - value)/2, label = ifelse(value != 0, as.character(variable), "")), size = 3, colour = "black", check_overlap = T) %+%
  expe_neuro_theme %+%
  ggtitle("expériences captées")

```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=10.5,fig.cap="carte de la répartition des expériences par neurone", out.width="1\\paperwidth"}
ggplot(gathered_expe_par_n[gathered_expe_par_n$id_neurone > 200 & gathered_expe_par_n$id_neurone < 300,], aes(x = id_neurone, y = value, fill = factor(variable))) %+% 
  geom_bar(stat = "identity", position = "fill") %+%
  #geom_text(aes(y = cumsum - 15, label = value), colour = "black") %+%
  expe_neuro_theme %+%
  ggtitle("expériences captées")

```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte de données par expérience", out.width="1\\paperwidth"}
ggplot(gathered_expe_par_n, aes(x = variable, y = value, fill = variable)) %+% 
  facet_wrap( ~ id_neurone, ncol = 30, scales = "free")  %+% 
  geom_bar(stat="identity") %+%
  expe_neuro_theme %+%
  ggtitle("expériences captées")

```
\elandscape
\restoregeometry

##Carte des temporalités captées par neurones

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte des neurones dans leur plan", out.width="1\\paperwidth"}
#on prend le df général
df_sec <- as.data.table(df_selec)
#on crée une var de facteur par secondes (arbitraire ??)
df_sec$one_sec_elapsed <- df_selec$tps.ecoule %>% round(digits = 0) %>% factor()
df_sec$five_sec_elapsed <- df_selec$tps.ecoule %>% `/`(5)  %>% round(digits = 0) %>% factor()
#on ajoute id_neurone
df_sec$id_neurone <- som$unit.classif

## Table of the time per neurones

#on retire les expe qui finissent pas 2 ou 3
#df_sec <- df_sec %>% filter(!grepl("*[2|3]",nom.experience))
#on retire le superflu, on regroupe par pair id_neurone - sec_elapsed...
#...et on compte le nombre de données associé à chaque pair
df_neuro_sec <- df_sec %>% select(id_neurone, one_sec_elapsed, nom.experience) 
df_neuro_5sec <- df_sec %>% select(id_neurone, five_sec_elapsed, nom.experience) 
df_count_neuro_1sec <- df_neuro_sec %>% 
  group_by(id_neurone, one_sec_elapsed, nom.experience) %>% summarise(count = n())
df_count_neuro_5sec <- df_neuro_5sec %>% 
  group_by(id_neurone, five_sec_elapsed, nom.experience) %>% summarise(count = n())

#on utilise tidyr::spread pour que chaque expe est sa colonne
expe_par_sec <- df_count_neuro_1sec %>% spread(nom.experience, count, fill = 0)
expe_par_5sec <- df_count_neuro_5sec %>% spread(nom.experience, count, fill = 0)

## table of the neurones graphs according to time
dt_neur_coord <- data.table(id_neurones = 1:900, x = som$grid$pts[,1], y = som$grid$pts[,2])

#create a data table that for each experience shows the neurone path in the data and the time spent in each neurone
chg_neur <- df_sec$id_neurone != lag(df_sec$id_neurone) | df_sec$nom.experience != lag(df_sec$nom.experience) 
chg_neur[1] <- T
path_neur <- df_sec$id_neurone[chg_neur]
nom_expe <- df_sec$nom.experience[chg_neur]
#no change : 1 as long as the referent is not changing
no_chg <- as.numeric(!chg_neur)
#reduce the no change vec into a vector that counts the length of each series of no change  
nb_data <- no_chg %>% purrr::reduce( function(prev, cur) { if(cur == 1) {prev[length(prev)] <- prev[length(prev)] + 1 ; prev } else{ prev[length(prev)+1] <- 1; prev }}, 1)
#creata a dt
dt_path_neur <- data.table(path_neur, nom_expe, nb_data)
dt_path_neur <- bind_cols(dt_path_neur,as.data.frame(dt_neur_coord[dt_path_neur$path_neur,.(x,y)]))
#next neurone coordinates
#let first of each experience at na ??
new_expe <- dt_path_neur$nom_expe != lag(dt_path_neur$nom_expe)
dt_path_neur$xend <- lag(dt_path_neur$x, default = dt_path_neur$x[1])
dt_path_neur$yend <- lag(dt_path_neur$y, default = dt_path_neur$y[1])
```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte des neurones dans leur plan", out.width="1\\paperwidth"}
#new graph of the neurones
ggplot(dt_neur_coord, aes(x = x, y = y)) + geom_point( col = "blue", shape = 21)
```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte des neurones en fonction du nombre de données captées par expérience", out.width="1\\paperwidth"}
ggplot(dt_path_neur, aes(x = x, y = y)) %+% 
  geom_point(aes(col = nom_expe, size = nb_data), alpha = 0.5) 

```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte des neurones par expérience avec \"jitter\"", out.width="1\\paperwidth"}
#graph with neurones color given according to the experience attached to it (add a little jitter to help with overlap)
ggplot(dt_path_neur, aes(x = x, y = y)) %+% geom_jitter(aes(col = nom_expe), width = 0.5, height = 0.5, alpha = 0.5) 

```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte du déplacement dans les neurones pour l'expérience AB", out.width="1\\paperwidth"}
test_AB <- dt_path_neur %>% filter(nom_expe == "AB") %>% dplyr::mutate(elapsed_time = cumsum(nb_data)) %>% data.table

ggplot(test_AB, aes(x = x, y = y)) %+% 
  geom_point(col = "darkgrey")  %+%
  geom_segment(aes(xend = xend, yend = yend, col = elapsed_time), alpha = .5, arrow = arrow(length = unit(0.02, "npc"))) %+%
  scale_color_gradient2(mid = "blue", high = "red")

```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo = F, cache = F, fig.height=8.5, fig.width=8.5,fig.cap="carte des référents par expérience", out.width="1\\paperwidth"}
dt_path_neur <- dt_path_neur %>% group_by(nom_expe) %>% mutate(elapsed_time = cumsum(nb_data)) %>% data.table() 

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(dt_path_neur, aes(x = x, y = y)) %+%
  ggplot2::facet_wrap(facets =  ~ nom_expe) %+% 
  scale_color_manual(values = cbPalette) %+%
  geom_point(aes(col = nom_expe)) 


```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo=F, cache = F, fig.height=8.5, fig.width=8.5, out.width="1\\paperwidth"}
graph_path_neurons <- function(df) {
  #colors & theme
  cbPalette2 <- c("#F00000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
  simple_theme <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                 panel.background = element_blank(), axis.line = element_line(colour = "black"))
  
  gg <-  ggplot(df, aes(x = x, y = y)) %+%
    facet_wrap(facets =  ~ nom_expe) %+% 
    geom_segment(aes(xend = xend, yend = yend, col = elapsed_time), alpha = .5, arrow = arrow(length = unit(0.05, "npc"))) %+% 
    scale_color_gradient2(mid = "grey87", high = "grey27") %+%
    geom_point(aes(fill = nom_expe, size = nb_data), shape = 21) %+% 
    scale_fill_manual(values = cbPalette2) %+%
    simple_theme %+%
    ggtitle("déplacement des données entre les référents")
  print(gg)
}

dt_path_short <- dt_path_neur %>% filter(elapsed_time < 10000)
graph_path_neurons(dt_path_short)
```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape

```{r, echo=F, cache = F, fig.height=8.5, fig.width=8.5, out.width="1\\paperwidth"}

dt_path_short2 <- dt_path_neur %>% filter(elapsed_time >= 10000 & elapsed_time < 20000)
graph_path_neurons(dt_path_short2)
```
\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape
```{r, echo=F, cache = F, fig.height=8.5, fig.width=8.5, out.width="1\\paperwidth"}

dt_path_short3 <- dt_path_neur %>% filter(elapsed_time >= 20000 & elapsed_time < 30000)
graph_path_neurons(dt_path_short3)
```

\elandscape
\restoregeometry

\newgeometry{left=0cm, right=0cm,bottom=3cm}
\newpage
\blandscape

```{r, echo=F, cache = F, fig.height=8.5, fig.width=8.5, out.width="1\\paperwidth"}
dt_path_short4 <- dt_path_neur %>% filter(elapsed_time >= 30000 & elapsed_time < 40000)
graph_path_neurons(dt_path_short4)
```

\elandscape
\restoregeometry
